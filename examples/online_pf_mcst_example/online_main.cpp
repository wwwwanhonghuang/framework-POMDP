#include <iostream>
#include <chrono>

#include <Eigen/Dense>

#include <pomdp/belief.hpp>
#include <pomdp/history.hpp>
#include <pomdp/history/sequence_history.hpp>

#include <pomdp/belief/pf_belief.hpp>
#include <pomdp/updater/particle_filter_updater.hpp>

#include <pomdp/planning/planner_runner.hpp>

#include <pomdp/planning/mcst/mcst_planner.hpp>
#include <pomdp/planning/mcst/pomcp_planner.hpp>
#include <pomdp/planning/mcst/ucb_selection.hpp>
#include <pomdp/planning/mcst/progressive_widening.hpp>
#include <pomdp/planning/mcst/composite_selection.hpp>
#include <pomdp/particle_filter/proposal_kernel.hpp>

#include "proposal.hpp"
#include <pomdp/particle_filter/systematic_resampler.hpp>

#include "model.hpp"
#include "action_sampler.hpp"
#include "belief_sampler.hpp"
#include "rollout.hpp"



using namespace online_example;



// çœŸå®ç³»ç»Ÿï¼ˆstate ä¸å¯è§ï¼‰
//     â†“ æ‰§è¡ŒåŠ¨ä½œ
// è§‚æµ‹ o_t
//     â†“
// Belief æ›´æ–°ï¼ˆParticle Filterï¼‰
//     â†“
// ç»™å®š (belief, history)
//     â†“
// Monte Carlo Tree Search è§„åˆ’
//     â†“
// é€‰ä¸€ä¸ªåŠ¨ä½œ

// Layer 0ï¼šçœŸå®ä¸–ç•Œï¼ˆæœ¬ä½“å±‚ï¼‰
    // true_state_t
    //     â†“ æ‰§è¡ŒåŠ¨ä½œ a_t
    // ç¯å¢ƒåŠ¨åŠ›å­¦
    //     â†“
    // (true_state_{t+1}, observation o_{t+1}, reward r_t)

// Layer 1ï¼šä¿¡å¿µæ›´æ–°ï¼ˆè®¤è¯†è®ºå±‚ï¼Œå¤–éƒ¨ï¼‰
    // (belief_t, history_t)
    //     + action a_t
    //     + observation o_{t+1}  [è¿™é‡Œçš„observationéç®—æ³•å†…éƒ¨observationï¼Œå¯èƒ½ç”±ä¼ æ„Ÿå™¨å¾—åˆ°ã€‚å¯¹äºplanningèŠ‚ç‚¹ï¼Œç®—æ³•å†…éƒ¨åœ¨è§„åˆ’æ—¶å­˜åœ¨å‡æƒ³observationï¼Œéœ€è¦è¿›è¡ŒåŒºåˆ†ã€‚]
    //     â†“
    // belief_{t+1}

        // è¿™æ˜¯ Bayes filtering
        // ä¸åœ¨ planner å†…
        // å¯æ›¿æ¢ä¸ºä»»ä½• belief è¡¨ç¤ºï¼ˆPF / Gaussian / learnedï¼‰


        // åœ¨ POMDP ä¸­ï¼Œ**æ ‡å‡†çš„è´å¶æ–¯æ›´æ–°**æ˜¯ï¼š
        // $$
        // \boxed{
        // b_{t+1}(s')
        // \;\propto\;
        // p(o_{t+1} \mid s', a_t)
        // \;\sum_{s}
        // p(s' \mid s, a_t)\, b_t(s)
        // }
        // $$
        // è§£é‡Šæ¯ä¸€é¡¹ï¼š

        // - $b_t(s)$ï¼šæ—§ beliefï¼ˆæˆ‘ä¹‹å‰å¯¹çŠ¶æ€çš„ç›¸ä¿¡ï¼‰
        // - $p(s' \mid s, a_t)$ï¼šçŠ¶æ€è½¬ç§»æ¨¡å‹
        // - $p(o_{t+1} \mid s', a_t)$ï¼šè§‚æµ‹æ¨¡å‹
        // - â€œ$\propto$â€ï¼šæœ€åè¦å½’ä¸€åŒ–

        // è¿™ä¸€æ­¥**å®Œå…¨åœ¨ planner å¤–éƒ¨å®Œæˆ**ã€‚  å¯ä½¿ç”¨ç²’å­æ»¤æ³¢è¿›è¡Œå®ç°ã€‚æ˜¯å¯¹è¯¥è¿‡ç¨‹çš„è’™ç‰¹å¡æ´›è¿‘ä¼¼ã€‚ â€œè’™ç‰¹å¡æ´›è¿‘ä¼¼â€å°±æ˜¯ï¼š
        // å½“ä¸€ä¸ªæœŸæœ› / ç§¯åˆ†ç®—ä¸å‡ºæ¥æ—¶ï¼Œ
        // ç”¨å¤§é‡éšæœºæ ·æœ¬çš„å¹³å‡æ¥è¿‘ä¼¼å®ƒã€‚

            // PF details
            // ## Step 1ï¼šé¢„æµ‹ï¼ˆPrediction / Proposalï¼‰

            // å¯¹æ¯ä¸ªç²’å­ï¼š
            // $$
            // x_{t+1}^i \sim p(s' \mid x_t^i, a_t)
            // $$
            // åœ¨ä½ çš„ä»£ç ä¸­å¯¹åº”ï¼š

            // ```
            // BootstrapProposal<State> proposal(model);
            // ```

            // è¯­ä¹‰æ˜¯ï¼š

            // > ç”¨ **çŠ¶æ€è½¬ç§»æ¨¡å‹** æŠŠç²’å­â€œå¾€å‰æ¨ä¸€æ­¥â€

            // ------

            // ## Step 2ï¼šè§‚æµ‹æ ¡æ­£ï¼ˆCorrection / Weightingï¼‰

            // ç”¨**çœŸå® observation** $o_{t+1}$ æ›´æ–°æƒé‡ï¼š
            // $$
            // w_{t+1}^i
            // \;\propto\;
            // p(o_{t+1} \mid x_{t+1}^i, a_t)
            // $$
            // ä»£ç è¯­ä¹‰ï¼ˆåœ¨ updater å†…éƒ¨ï¼‰ï¼š

            // ```
            // weight *= observation_likelihood(x_next, a, obs)
            // ```

            // ğŸ‘‰ è¿™æ˜¯ **è´å¶æ–¯å…¬å¼ä¸­çš„ä¼¼ç„¶é¡¹**

            // ------

            // ## Step 3ï¼šå½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

            // $$
            // \sum_i w_{t+1}^i = 1
            // $$

            // ä½ åœ¨ `ParticleBelief::normalize()` ä¸­å·²ç»æä¾›äº†æ¥å£ã€‚

            // ------

            // ## Step 4ï¼šé‡é‡‡æ ·ï¼ˆResamplingï¼Œå¯é€‰ï¼‰

            // å½“æœ‰æ•ˆæ ·æœ¬æ•°ï¼ˆESSï¼‰è¿‡ä½ï¼š
            // $$
            // \text{ESS} = \frac{1}{\sum_i (w_i)^2}
            // $$
            // å°±æ‰§è¡Œé‡é‡‡æ ·ï¼Œé˜²æ­¢ç²’å­é€€åŒ–ï¼š

// Layer 2ï¼šè§„åˆ’æ ‘çš„å½¢æ€ï¼ˆå½¢æ€å­¦å±‚ï¼‰
    // èŠ‚ç‚¹ = history h
    // è¾¹ = action a â†’ observation o
    // æ¯ä¸ª history è¯­ä¹‰ç­‰ä»·äºä¸€ä¸ª belief

        // è¿™æ˜¯ POMDP çš„å¤©ç„¶å†³ç­–ç©ºé—´

        // ä¸ MCSTã€ç®—æ³•ã€ä»£ç å®ç°æ— å…³

// Layer 3ï¼šè§„åˆ’è¡Œä¸ºï¼ˆè¡Œä¸ºå­¦å±‚ï¼Œplanner / MCSTï¼‰
    // input belief_{t+1}, history_{t+1}

    // 1. ä» belief é‡‡æ ·ä¸€ä¸ª state s
    // 2. ä»æ ¹ history å‘èµ·ä¸€æ¬¡ simulate(s, h, depth=0)
    // 3. simulate ä¸­ï¼š
    //    - selectionï¼ˆPW / UCBï¼‰
    //    - expansionï¼ˆæ–° action / historyï¼‰
    //    - simulationï¼ˆgenerative model äº§ç”Ÿ s', o, rï¼‰ generative model æ˜¯ä¸€ä¸ªåœ¨â€œå†³ç­–ç›¸å…³å±‚é¢â€ä¸Šè¿‘ä¼¼çœŸå®ä¸–ç•ŒåŠ¨åŠ›å­¦ä¸æ„ŸçŸ¥è¿‡ç¨‹çš„å¯é‡‡æ ·æ¨¡å‹ï¼Œç”¨ä»¥æ¨¡æ‹ŸçŠ¶æ€ã€è§‚æµ‹ä¸å›æŠ¥çš„è”åˆæ¼”åŒ–ã€‚
    //    - rolloutï¼ˆæ ‘å¤–é»˜è®¤ç­–ç•¥ï¼‰
    //    - backupï¼ˆæ›´æ–° Q, Nï¼‰

// Layer 4ï¼šåŠ¨ä½œå†³ç­–ï¼ˆæ¥å£å±‚ï¼‰
    // planner è¾“å‡ºçš„å”¯ä¸€ç»“æœï¼š
    // a* = argmax_a Q(h_root, a)


        // planneræ˜¯åœ¨è¿›è¡Œ ç»™å®š (belief, history) â†“ Monte Carlo Tree Search è§„åˆ’ â†“ é€‰ä¸€ä¸ªåŠ¨ä½œ çš„æ­¥éª¤ã€‚
        // planner åœ¨â€œè§„åˆ’æ ‘â€ä¸­è¿›è¡Œæ‹“å±•ï¼Œ
        // è¿™æ£µæ ‘åªå­˜åœ¨äº planner å†…éƒ¨ï¼Œæ˜¯ä¸€ä¸ªâ€œå‡æƒ³æ ‘â€ã€‚

// â€œåŸºäº history çš„è§„åˆ’æ ‘ï¼ˆhistoryâ€“actionâ€“historyï¼‰â€

// å½¢å¼æ˜¯ï¼š

// hâ‚€
//  â”œâ”€â”€ aâ‚
//  â”‚    â””â”€â”€ hâ‚ = hâ‚€ + (aâ‚, oâ‚)
//  â”œâ”€â”€ aâ‚‚
//  â”‚    â””â”€â”€ hâ‚‚ = hâ‚€ + (aâ‚‚, oâ‚‚)

// åœ¨ POMCP / MCST ä¸­ï¼Œæ ‘æ˜¯ ä¸¤ç±»èŠ‚ç‚¹äº¤æ›¿å‡ºç°çš„ï¼š

// History Node (belief node)
//     â†“ choose action
// Action Node
//     â†“ sample transition
// History Node
//     â†“ choose action
// Action Node
//     ...

// UCB åªåœ¨â€œåŒä¸€ä¸ª history node ä¸‹çš„ action nodesâ€ä¹‹é—´é€‰

int main() {
    // ------------------------------------------------------------
    // 3. Samplers
    // ------------------------------------------------------------
    ContinuousActionSampler action_sampler(/*a_max=*/1.0);
    PFBeliefSampler<State> belief_sampler;


    // ------------------------------------------------------------
    // 1. Model (ground truth + generative)
    // ------------------------------------------------------------
    ContinuousModel model(action_sampler, /*dt=*/0.1);

    // ------------------------------------------------------------
    // 2. Initial belief (particle filter)
    // ------------------------------------------------------------
    constexpr std::size_t NUM_PARTICLES = 500;


    pomdp::ParticleBelief<State> belief;
    belief.particles.reserve(NUM_PARTICLES);

    for (std::size_t i = 0; i < NUM_PARTICLES; ++i) {
        State x(4);
        x.setZero();

        pomdp::Particle<State> p;
        p.x = x;
        p.weight = 1.0 / NUM_PARTICLES;

        belief.particles.push_back(p);
    }

    BootstrapProposal<State> proposal(model);
    pomdp::SystematicResampler<State> resampler;

    // PF updater (uses model's kernels)
    pomdp::ParticleFilterUpdater<State> updater(
        model,
        proposal,
        resampler,
        /*ess_threshold=*/0.5
    );

 

    // ------------------------------------------------------------
    // 4. Rollout policy
    // ------------------------------------------------------------
    Rollout rollout([&]() {
        return action_sampler.sample_random_action();
    });
    // ------------------------------------------------------------
    // 5. Selection strategy (PW + UCB)
    // ------------------------------------------------------------
    auto pw = std::make_shared<pomdp::mcst::ProgressiveWidening>(
        action_sampler,
        /*k=*/1.0,
        /*alpha=*/0.5
    );

    auto ucb = std::make_shared<pomdp::mcst::UCBSelection>(
        /*c=*/1.4
    );

    pomdp::mcst::CompositeSelection selection;
    selection.add_strategy(pw);
    selection.add_strategy(ucb);

    // ------------------------------------------------------------
    // 6. Planner (MCST or POMCP)
    // ------------------------------------------------------------
    constexpr std::size_t HORIZON = 15;
    constexpr double DISCOUNT = 0.95;

    pomdp::mcst::MCSTPlanner<State> planner(
        belief_sampler,
        action_sampler,
        model,
        selection,
        rollout,
        HORIZON,
        DISCOUNT
    );

    // Online / anytime runner
    pomdp::PlannerRunner runner(planner);

    // ------------------------------------------------------------
    // 7. Execution loop (online)
    // ------------------------------------------------------------
    pomdp::SequenceHistory history;

    pomdp::Action action = action_sampler.sample_random_action();


    State true_state(4);
    true_state.setZero();

    constexpr std::size_t NUM_STEPS = 100;

    for (std::size_t t = 0; t < NUM_STEPS; ++t) {
        // --------------------------------------------------------
        // Observe environment
        // --------------------------------------------------------
        auto sim_result = model.step(true_state, action);
        true_state = sim_result.next_state;
        const pomdp::Observation& obs = sim_result.observation;

        // --------------------------------------------------------
        // Belief update (external to planner)
        // --------------------------------------------------------
        const pomdp::Action prev_action = action;

        auto new_belief = updater.update(
            belief,
            prev_action,
            obs,
            history
        );
        belief = *dynamic_cast<pomdp::ParticleBelief<State>*>(new_belief.release());

        // --------------------------------------------------------
        // Online planning (time-bounded)
        // --------------------------------------------------------
        runner.run_for_duration(
            belief,
            history,
            std::chrono::milliseconds(10)
        );

        // --------------------------------------------------------
        // Select best action anytime
        // --------------------------------------------------------
        action = planner.best_action();

        // --------------------------------------------------------
        // Logging
        // --------------------------------------------------------
        const auto& u = action_sampler.action_value(action);

        std::cout
            << "t=" << t
            << "  true_pos=("
            << true_state[0] << ", "
            << true_state[1] << ")"
            << " action=("
            << u[0] << ", "
            << u[1] << ") "
            << std::endl;

        // --------------------------------------------------------
        // Update history
        // --------------------------------------------------------
        
        history.append(action, obs);
    }

    return 0;
}
